저희는 an image is worth 16x16 words : transformer for image recognition at scale을 소개해드립니다.

저희가 논문에서 잡은 키워드는 inductive bias, fine-tuning, higher resolutio, few-shot, 자기 지도학습입니다.

먼저 introduction을 소개해드리겠습니다.

논문들을 살펴보게되면 예전 논문들에서는 cnn에 self attention을 어떻게 적용할지에 대한 고민을 많이하였다면 이후에는 transformer 모델 자체를 이용해보고자하는 논문들이 많은 것을 알 수 있습니다.

논문의 introduction을 살펴보면 (내용으로 읽기)

다음은 Proposed Method를 살펴보겠습니다

ViT 모델 구조를 살펴보면(내용으로 읽기)

ViT의 작동과정은 총 5가지로 나눌 수 있는데요
이미지가 이렇게 있다고 하면 pxp 패치로 나누어 분할하는 과정을 거치고 보시는 것처럼 9개의 패치 sequence로 구축합니다.
두번째 스텝에서는 이 패치들을 왼쪽 위부터 오른쪽 아래까지 순서대로 패치를 받아서 패치에 대해서 Trainable linear projection을 거치게 됩니다. 이 trainable linear projection의 역할은 각각의 패치 중에 첫번째 xp를 flatten한 벡터를 D차원으로 변환을 하는 역할을 한다. 이렇게 step1에서 분할이된 총 9개의 패치는 각각이 이렇게 쭉 flatten한 vector를 D차원으로 변환하는 과정이 step2에서 이루어집니다. 이렇게 step2를 거치고 나면은 하나의 패치가 여기 분홍색으로 표현된 vector로 표현됩니다.

step2까지 진행되고 이후 step3에서는 learnable class embedding, 앞에 보이시는  앞에 별표에 해당하는 learnable class embedding이라는 것이 붙게 되고 이런 learnable class embedding과 그리고 step2에서 나온 총 9개의 패치 임베딩에 learnable position 임베딩을 더하는 과정이 이루어집니다. 저희가 알고 있는 일반적인 transformer와 동일하게 position embedding까지 더한 input이 준비가 되고 나면은 이거를 순서대로 transformer encoder에 넣어가지고 마지막 layer에서 class embedding에 대한 output인 image representation을 도출하는 step4를 거치게 됩니다. 이 부분은 bert에서 하는 것과 동일하게 class torken을 넣고 거기서 나온 클래스 토큰의 위치에 해당하는 representation을 사용하게 되는데 그 과정과 동일하게 진행된다고 보시면 되겠습니다. 이런 식으로 스텝4에서 transformer encoder를 통해서 class embedding의 위치에 있는 image representation을 도출한 후에 마지막으로 step5에서는 MLP Head에다가 step4에서 도출된 image representation을 input으로 넣어서 이미지의 class를 분류하는 과정을 거치게 됩니다. 
이런식으로 step1부터 5까지 이미지가 존재할 때 그 이미지를 패치로 분할한 후에 이제 시퀀스 그러니까 순서가 있는 시퀀스를 사용을 해서 이렇게 이미지를 분류한다고 보시면 될 것 같습니다.

다음은 transformer encoder에 대해서 좀더 수학 공식을 넣어서 좀더 상세하게 알아보도록 하겠습니다. (내용으로 읽기) 예를 들어 (3, 256, 256) 크기의 이미지를 입력으로 받고 p 그러니까 패치의 크기를 16으로 사용한다면, 각 패치의 크기는 (3, 16, 16)이 되고 패치의 객수는 16 x 16개가 됩니다.  이 패치를 flatten하게 되면 3*16*16 = 768이므로 768크기의 벡터를 16 x 16개 가지게 됩니다. 이 값을 시퀀스 데이터로 나타내면 (256, 768)의 형태로 표현할 수 있습니다.

이제 앞에서 생성한 xp 즉 시퀀스를 Embedding 하기 위하여 행렬 E와 연산을 해줍니다. (내용 그대로 읽기)

(내용 그대로 읽기)

지금까지 과정을 CIFAR-10 데이터 예시로 다시 한번 살펴보면 다음과 같습니다.
CIFAR-10 = (3, 32, 32)이고 P를 4라고 하면 나뉘어진 패치의 갯수인 N은 32*32 / (4*4) = 64
(그대로 읽기) 

(그대로 읽기)

(그대로 읽기)

논문에 나와있는 식을 보면 LM(LayerNorm), MSA, MLP 연산을 조합하게 되면 Transformer Encoder을 구현할 수 있다고 보시면 될 것 같습니다.

(그대로 읽기)

저희가 이제 batch Normalization에 대해서는 많이 들었는데 Layer Normalization은 많이 못들어본 것 같아서 제가 한번 찾아보았습니다.
먼저 세로 방향으로 [1,2,0,4,5,1]이 하나의 sample(x1)입니다.
마찬가지로 [3,2,1,6,2,0]과 [6,2,5,1,3,1]도 각각 sample(x2, x3)입니다.
그렇다면 가로 방향으로 [[1],[3],[6]]은 각 x1, x2, x3의 첫번째 feature가 됩니다.
다시 말해서 x11, x12, x31 이라고 보시면 될 것 같습니다.
이렇게 말씀드리면 이해가 되실 것 같습니다.
정리하면 batch normalization에서 batch에 있는 "모든" sample들에 대해서 "각 feature의 평균과 분산"을 구하여 정규화하는 것이고
Layer Normalization은 "각 sample에 대해서""feature들(xi1,xi2.....xid)에 대해서 평균과 분산을 구해서 정규화하는 것입니다.
수식을 살펴보게 되면 확실해집니다.
먼저 batch normalization을 보시면
M은 batch size입니다. 이 수식은 batch안의 모든 sample들에 대해서 k번째 feature의 평균과 분산을 구하는 것입니다.
다음은 layer normalization을 살펴보시면
k는 sample의 dimension입니다.
아까 설명한대로 수식에서는 하나의 sample xi의 모든 feature(1...K)의 평균과 분산을 구하는 것입니다.

이제 MUlti-head attention에 대하여 알아보도록 하겠습니다.

(글 그대로 읽기)
q, k, v를 한번에 연산하기 위해서 4번째 식을 사용하기도 합니다.
q,k,v matrix를 만들어주기 위한 차원 변경을 수행합니다.
Dh는 보통 D/k로 설정하며(여기서 k는 어텐션 헤드의 개수)이는 파라미터 개수를 head개수에 무관하게 동일하도록 하기 위함입니다.

첫번째 식은 Attention 가중치 A 계산을 하는 것이고 두번째는 Attention 가중치 A로 v의 가중합 계산이라고 보시면 될 것 같습니다.
(그대로 읽기)

(그대로 읽기)

마지막으로 MLP과정을 거치고, 이때, GELU Activation을 사용합니다.
GELU는 입력값과 입력값의 누적 정규 분포의 곱을 사용한 형태입니다.
최근 BERT, Wave2Vec2.0같은 최신 논문에서 많이 사용하는 성능이 제일 뛰어난 함수입니다.
(그대로 읽기)

(그대로 읽기)

이제 vit의 디테일한 요소들에 대해서 알아보도록 하겠습니다.
먼저 Positional Embedding에 대해서 살펴보면 ViT에서는 아래 4가지 Positional embedding을 시도한 후, 최종적으로 가장 효과가 좋은 1D position embedding을 ViT에 사용하였습니다.
그러니까 여기서 시도한 Positional Embedding이 총 4가지인데 
첫번째는 No positional information입니다. 말 그대로 여기서는 Positional embedding을 사용하지 않고 patch만 transformer input으로 사용한 경우 입니다.
두번째는 1-dimensional positional embedding으로 이 input sequence 즉, patch를 raster order라고 하여 예시에서 보여드린 것처럼 3차원의 이미지가 존재를 할 때 가로와 세로 depth가 존재하는데 이 가로와 세로를 기준으로 봤을 때 왼쪽 위부터 오른쪽 아래까지 순서대로 patch를 보는 방식을 raster order이고
세번째는 2-dimensional positional embedding은 3차원의 이미지에 대해서 가로와 세로 축 2가지에 대해서 dimensional grid를 만들어가지고 x축과 y축에 대한 좌표가 있는 positional embedding이라고 보시면 되겠습니다.
마지막으로 relative positional embedding의 경우에는 patch들 사이의 상대적인 거리를  사용한 positional embedding이라고 보시면 될 것 같습니다.
이렇게 총 4가지로 실험을 진행했을 때 1-dimensional positional embedding이 좋은 효과를 보였기 때문에 이 ViT에서 최종적으로 Learnalble positional embedding에다가 1-dimensional positional embedding을 사용했다고 보시면 되겠습니다.

다음은 Inductive Bias에 대해서 알아보도록 하겠습니다.
Inductive Bias는 Training에서 보지 못한 데이터에 대해서도 적절한 귀납적 추론이 가능하도록 하기 위해 모델이 가지고 있는 가정들의 집합을 의미합니다.
딥러닝의 기본적인 요소들의 Inductive bias에 대해 살펴보겠습니다.
(그대로 읽기)
ViT에서 MLP는 Locality와 translation equivariance가 있지만 MSA는 global하기 때문에 cnn보다 Image-specific inductive bias가 낮습니다.

좀더 구체적으로 inductive bias의 역할에 대해 알아보도록 하겠습니다.
(그대로 읽기)

(그대로 읽기)

앞에 말씀드린 것처럼 ViT에서는 image-specific inductive bias가 낮기 때문에 따로 주입을 하려는 시도를 합니다.
첫번째로 Patch extraction 제일 처음에 봤던 이미지를 여러개의 패치로 분할해서 순서가 존재하는 형태로 넣는 것이 하나의 시도위한 방법 이 덕에 ViT의 mlp는 다른 mlp들과는 다르게 아무래도 locality와 translation equivariance를 가지고 있다 볼 수 있습니다.
두번째 장치는 resolution adjustment이고 이부분은 finetuning을 할 때 사용하는데 이 패치의 크기는 동일하게 사용하다보니 이미지에 크기에 따라서 resolution에 따라서 패치의 크기는 동일하지만 생성되는 패치의 갯수는 달라지게 되는데  이렇게 달라지는 갯수에 대해서 이제 finetunning을 할 때 potional embedding을 조금 조정하는 부분이 있는데 이부분 덕분에 inductive bias가 주입이 좀 되었다고 볼 수 있습니다.

다음은 Hybrid architecture입니다.
ViT는 아까 설명을 드렸던 것처럼 raw image를 패치로 나누어서 받고 있는데 이걸 raw image가 아닌 CNN으로 추출해서 raw image의 feature map을 활용하는 hybrid architecture로도 사용할 수 있다고 언급합니다. Feature map은 이미 raw image의 공간적 정보를 추출을 한 상황이기 때문에 공간적 정보를 포함하고 있으므로 hybrid archtecture는 패치 크기를 1X1으로 설정해도 이미 locality를 가지고 있다고 보시면 될 것 같습니다.
(3번째는 그대로 읽기)

그리고 이제 기본적으로 raw -image에 사용하는 ViT 또는 Raw image에 cnn으로 추출한 feature map에 적용하는 hybrid architecture 이런 2가지 중에 하나로 pre-train을 진행한 후에 해당 모델을 downstream task에 fine-tuning을 사용하여야하는데 그 부분에 대해서 살펴보겠습니다. 
(2부터 그대로 읽기)

(그대로 읽기)

(그대로 읽기)

(그대로 읽기)

(그대로 읽기)

다음은 experiment입니다.

먼저 dataset같은 경우에는
(그대로 읽기)

각각의 classes의 갯수와 image의 갯수는 표에 나와있는 것과 동일합니다.
이미지는 1k와 각각 이미지 갯수와 class갯수가 차이가 나는 것을 볼 수 있습니다.
여기 논문에서는 이 3가지를 가지고 pretrain을 진행을 하고 benchmark task들을 downstream task를 해서 pre-trained ViT의 representation 성능을 검증합니다.

다음 모델 variants에 대해 말씀드리면 총 3개의 volume에 대하여 실험을 진행하였습니다. 여기서는 모델 3가지가 ViT-Base / ViT-Large / ViT-Huge라고 볼 수 있습니다.
각각 모델에 대해서 다양한 패치 크기로 실험이 진행되었습니다.
baseline CNN같은 경우에는 (그대로 읽기)

(그대로 읽기)









 




